#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Jan  8 11:54:08 2023

@author: aixuexi
"""
from __future__ import absolute_import
from __future__ import print_function
import matplotlib.pyplot as plt
from matplotlib import rcParams

import autograd.numpy as np
import autograd.numpy.random as npr
import autograd.scipy.stats.multivariate_normal as mvn
import autograd.scipy.stats.norm as norm

from autograd import grad
from autograd.misc.optimizers import adam

import pickle
import time
import math
import multiprocessing
import prettytable as pt
from sklearn.metrics import mean_squared_error # 均方误差
from sklearn.metrics import mean_absolute_error # 平方绝对误差
from sklearn.metrics import r2_score # R square
from scipy.stats import spearmanr, pearsonr, ttest_rel

from scipy.stats import t, chi2, f  # t分布, chi2
import scipy


#%%
def sampling_f(m, n, num_samples):
    # 抽取f分布
    rv = f(m, n)
    return rv.rvs(size=num_samples)[:, np.newaxis]


def sampling_chi2(df, num_samples):
    # 抽取chi square分布
    rv = chi2(df)
    return rv.rvs(size=num_samples)[:, np.newaxis]


def sampling_t(df, num_samples):
    # 抽取t分布
    rv = t(df)
    return rv.rvs(size=num_samples)[:, np.newaxis]


def sampling_normal(mean, log_std, num_samples):
    # 抽取正态分布
    rs = npr.RandomState()
    samples = rs.randn(num_samples, 1) * np.exp(log_std) + mean
    return samples


def sampling_uniform(low, high, num_samples):
    # 抽取均匀分布
    rs = npr.RandomState()
    samples = rs.uniform(0, 1, size=(num_samples, 1)) * (high-low) + low
    return samples


def sampling_poisson(lam, num_samples):
    # 抽取泊松分布
    rs = npr.RandomState()
    samples = rs.poisson(lam, num_samples)
    return samples


def smapling_binomial(n, p, num_samples):
    # 抽取两点分布
    rs = npr.RandomState()
    samples = rs.binomial(n, p, num_samples)
    return samples


def sigmoid(x):
    return 1 / (1 + np.exp(-x))


def arc_sigmoid(x):
    # sigmoid的逆函数
    x = np.minimum(np.maximum(x, 0.05), 1)
    y = -np.log(np.maximum(1/x - 1, 1e-2))
    return y


def uniform_logpdf(x_low, x_high, z_e_):
    # Log of the probability density function evaluated at x
    # 属于 区间外是 0 概率 (1e-5去替代, log1e-5)
    overflow_high = z_e_ >= x_high
    overflow_low  = z_e_ <= x_low
    non_overflow  = 1- overflow_high - overflow_low
    # 属于 [x_low, x_high] 是均匀分布
    epsilon = 100
    pdf_overflow_high = overflow_high * 1 / (epsilon * np.maximum(z_e_-x_high, 1e-2)) # 要求概率小 <= 0.1
    pdf_overflow_low  = overflow_low  * 1 / (epsilon * np.maximum(x_low-z_e_,  1e-2)) # 要求概率小 <= 0.1
    pdf_non_overflow  = non_overflow  * (z_e_-x_low) / np.maximum(x_high-x_low, 1e-2)
    
    logpdf =  np.log(pdf_overflow_high + pdf_overflow_low + pdf_non_overflow)
    return logpdf


def plot_sigmoid(l, h):
    # 不同的lam_pi, Uniform(LOW, HIGH)
    fig = plt.figure(figsize=(10, 8))
    plt.rcParams['savefig.dpi'] = 300
    plt.rcParams['figure.dpi'] = 300
    config = {
              "font.family" : "Times New Roman",
              "font.size" : 25
              }
    rcParams.update(config)
    
    x_low  = max(l, -5)
    x_high = min(h + l, 5)
    x = np.arange(-5, 5.25, 0.25)
    y = 2 * sigmoid(x)
    plt.plot(x, y, label='Sigmoid', c='blue')
    plt.plot([-5, x_low],  [2 * sigmoid(x_low),  2 * sigmoid(x_low)], c='green', linestyle='--', linewidth=3, label="LOW")
    plt.plot([-5, x_high], [2 * sigmoid(x_high), 2 * sigmoid(x_high)], c='red', linestyle='--', linewidth=3, label="HIGH")
    plt.xticks(np.arange(-5, 6, 1))
    plt.yticks(np.arange(0, 2.5, 0.5))
    plt.xlim(x[0], x[-1])
    plt.ylim(-0, 2)
    plt.legend(frameon=False, loc='lower right')


def log_p_zx_density(data_tuple, z_tuple, model_params_tuple):

    x_arr, pi_arr, mask_arr = data_tuple 
    z_q, z_e = z_tuple
    model_params_e, model_params_q1, model_params_P = model_params_tuple
    
    model_params_l  = model_params_e[:, 0]
    model_params_h  = model_params_e[:, 1]
    mu_Q, log_sig_Q = model_params_q1[0]
    mu_P, log_sig_P = model_params_P[0]
    
    # 计算 p_obs
    M_pi_e = pi_arr * z_e[:, :, np.newaxis, :]
    M_pi_e = np.mean(M_pi_e, axis=-1)
    qe     = z_q[:, :, np.newaxis] + M_pi_e
    p_obs  = x_arr - qe
    
    # 隐变量生成概率 P(Z_q)
    log_Q_density = norm.logpdf(z_q, mu_Q, np.exp(log_sig_Q))
    
    # 隐变量生成概率 P(Z_e)
    aid_num, num_samples, pi_num = z_e.shape
    
    z_e_   = np.transpose(z_e, (1, 0, 2))
    z_e_   = z_e_.reshape(num_samples, -1)
    l      = np.ones((aid_num, 1)) * model_params_l
    h      = np.ones((aid_num, 1)) * model_params_h
    l      = l.flatten()
    h      = h.flatten()
    log_pi_density = norm.logpdf(z_e_, l, np.exp(h))
   
    # 观测数据生成概率 P(X|Z; M)
    log_P_density = norm.logpdf(p_obs, 0, np.exp(log_sig_P))
    log_P_density = log_P_density * mask_arr
    log_P_density = np.sum(log_P_density, axis=-1)
    
    # q研究能力概率 + e策略效率概率 + p引用随机性概率
    logpq = np.sum(log_Q_density, axis=0) + np.sum(log_pi_density, axis=-1) +\
            np.sum(log_P_density, axis=0)
    return logpq


#%%
def Estep(data, model_params, var_params, num_samples, step_size, num_iters):
                                          
    def variational_objective(var_params_cat, t):
        """Provides a stochastic estimate of the variational lower bound."""
        var_params_q_ = var_params_cat[: len_q]
        var_params_e_ = var_params_cat[len_q:]
        var_params_q  = var_params_q_.reshape(q_shape)
        var_params_e  = var_params_e_.reshape(e_shape)
        
        e_log_std     = np.maximum(var_params_e[:, :, 1], -5) # 防止溢出
        e_log_std     = np.minimum(e_log_std, 1)              # 防止溢出
        
        # 模型参数
        model_params_e     = model_params[:pi_num]        # 策略效率模型参数
        model_params_q1    = model_params[pi_num:-1]      # 科研能力模型参数
        model_params_P     = model_params[-1:]            # 引用范式随机性参数
        model_params_tuple = (model_params_e, model_params_q1, model_params_P)
        
        # padding 操作 - 人员数目 * 样本数目 * 最大文章数目
        nop_list = list()
        for x in x_list:
            nop = len(x)
            nop_list.append(nop)
        max_nop = max(nop_list)
        
        x_list_pad_j  = list()                           # 将机构j人员的发文量补齐为最大值max_nop
        pi_list_pad_j = list()                           # 补齐
        mask_j        = list()                           # 标记那些位置是补齐
        for x, pi in zip(x_list, pi_list):
            nop = len(x)
            #
            x         = np.ones((num_samples, 1)) * x
            x_zeros   = np.zeros((num_samples, max_nop-nop))
            x_padding = np.concatenate([x, x_zeros], axis=-1)
            x_list_pad_j.append(x_padding)
            #
            pi         = np.ones((num_samples, 1, 1)) * pi
            pi_zeros   = np.zeros((num_samples, max_nop-nop, pi_num))
            pi_padding = np.concatenate([pi, pi_zeros], axis=-2)
            pi_list_pad_j.append(pi_padding)
            #
            mask  = np.concatenate([np.ones(nop), np.zeros(max_nop-nop)])
            mask  = np.ones((num_samples, 1)) * mask
            mask_j.append(mask)
        x_arr    = np.array(x_list_pad_j)               # 人员数目 * 样本数目 * 最大文章数目
        pi_arr   = np.array(pi_list_pad_j)              # 人员数目 * 样本数目 * 最大文章数目 * 策略数目
        mask_arr = np.array(mask_j)                     # 人员数目 * 样本数目 * 最大文章数目
        data_tuple = (x_arr, pi_arr, mask_arr)
        
        # 抽样研究能力(q)变分参数 - 人员数目 x 样本数目
        z_q  = sampling_normal(var_params_q[:, 0], var_params_q[:, 1], num_samples).T
        
        # 抽样策略效率(e)变分参数 - 人员数目 x 样本数目 x 策略数目
        z_e_ = sampling_normal(var_params_e[:, :, 0].flatten(),
                              np.exp(e_log_std.flatten()),
                              num_samples)
        z_e  = z_e_.reshape((num_samples, aid_num, pi_num))
        z_e  = np.transpose(z_e, (1, 0, 2))
        z_tuple = (z_q, z_e)                              # 隐变量 (研究能力 & 策略效率)
        
        # Black box variational inference 内的 ELBO表达式:Eq[logp - logq]
        logpq = log_p_zx_density(data_tuple, z_tuple, model_params_tuple)
        part1 = np.mean(logpq)
        
        part2_z_q = norm.logpdf(z_q.T, var_params_q[:, 0], np.exp(var_params_q[:, 1]))
        part2_z_e = norm.logpdf(z_e_, 
                                var_params_e[:, :, 0].flatten(), 
                                np.exp(e_log_std.flatten()))
        part2     = np.sum(part2_z_q, axis=-1) + np.sum(part2_z_e, axis=-1)
        part2     = np.mean(part2)
        
         # 求ELBO最大, 所以这里加个负号即minimize
        lower_bound = part1 - part2 
        return -lower_bound
    
    # 变分参数
    var_params_q, var_params_e = var_params
    q_shape                    = var_params_q.shape
    e_shape                    = var_params_e.shape
    aid_num, q_dim             = q_shape
    aid_num, pi_num, e_dim     = e_shape
    var_params_q_              = var_params_q.flatten()
    var_params_e_              = var_params_e.flatten()
    len_q                      = len(var_params_q_)
    len_e                      = len(var_params_e_)
    var_params_cat             = np.concatenate([var_params_q_, var_params_e_])
    
    # 观测数据
    x_list  = list()
    pi_list = list()
    for k in data:
        x_k = data[k]['x']
        pi_k = data[k]['pi']
        x_list.append(x_k)
        pi_list.append(pi_k)
    
    # 梯度下降更新变分参数 - q & e
    gradient            = grad(variational_objective)
    # gradient(var_params_cat, 0)
    var_params_cat_next = adam(gradient, var_params_cat, step_size=step_size, num_iters=num_iters)
    var_params_q_next   = var_params_cat_next[: len_q].reshape(q_shape)
    var_params_e_next   = var_params_cat_next[len_q:].reshape(e_shape)
     
    var_params_e_next[:, :, 1] = np.maximum(var_params_e_next[:, :, 1], -5)  # 防止溢出
    var_params_e_next[:, :, 1] = np.minimum(var_params_e_next[:, :, 1], 1)   # 防止溢出
    var_params_next     = (var_params_q_next, var_params_e_next)
    return var_params_next


def Mstep(data, model_params, var_params, num_samples, step_size, num_iters):
              
    def variational_objective(model_params, t):
        """Provides a stochastic estimate of the variational lower bound."""
        var_params_q_ = var_params_cat[: len_q]
        var_params_e_ = var_params_cat[len_q:]
        var_params_q  = var_params_q_.reshape(q_shape)
        var_params_e  = var_params_e_.reshape(e_shape)
        
        e_log_std     = np.maximum(var_params_e[:, :, 1], -5) # 防止溢出
        e_log_std     = np.minimum(e_log_std, 1)              # 防止溢出
        
        # 模型参数
        model_params_e     = model_params[:pi_num]        # 策略效率模型参数
        model_params_q1    = model_params[pi_num:-1]      # 科研能力模型参数
        model_params_P     = model_params[-1:]            # 引用范式随机性参数
        model_params_tuple = (model_params_e, model_params_q1, model_params_P)
        
        # padding 操作 - 人员数目 * 样本数目 * 最大文章数目
        nop_list = list()
        for x in x_list:
            nop = len(x)
            nop_list.append(nop)
        max_nop = max(nop_list)
        
        x_list_pad_j  = list()                           # 将机构j人员的发文量补齐为最大值max_nop
        pi_list_pad_j = list()                           # 补齐
        mask_j        = list()                           # 标记那些位置是补齐
        for x, pi in zip(x_list, pi_list):
            nop = len(x)
            #
            x         = np.ones((num_samples, 1)) * x
            x_zeros   = np.zeros((num_samples, max_nop-nop))
            x_padding = np.concatenate([x, x_zeros], axis=-1)
            x_list_pad_j.append(x_padding)
            #
            pi         = np.ones((num_samples, 1, 1)) * pi
            pi_zeros   = np.zeros((num_samples, max_nop-nop, pi_num))
            pi_padding = np.concatenate([pi, pi_zeros], axis=-2)
            pi_list_pad_j.append(pi_padding)
            #
            mask  = np.concatenate([np.ones(nop), np.zeros(max_nop-nop)])
            mask  = np.ones((num_samples, 1)) * mask
            mask_j.append(mask)
        x_arr    = np.array(x_list_pad_j)               # 人员数目 * 样本数目 * 最大文章数目
        pi_arr   = np.array(pi_list_pad_j)              # 人员数目 * 样本数目 * 最大文章数目 * 策略数目
        mask_arr = np.array(mask_j)                     # 人员数目 * 样本数目 * 最大文章数目
        data_tuple = (x_arr, pi_arr, mask_arr)
        
        # 抽样研究能力(q)变分参数 - 人员数目 x 样本数目
        z_q  = sampling_normal(var_params_q[:, 0], var_params_q[:, 1], num_samples).T
        
        # 抽样策略效率(e)变分参数 - 人员数目 x 样本数目 x 策略数目
        z_e_ = sampling_normal(var_params_e[:, :, 0].flatten(),
                              np.exp(e_log_std.flatten()),
                              num_samples)
        z_e  = z_e_.reshape((num_samples, aid_num, pi_num))
        z_e  = np.transpose(z_e, (1, 0, 2))
        z_tuple = (z_q, z_e)                              # 隐变量 (研究能力 & 策略效率)
        
    
        # Black box variational inference 内的 ELBO表达式:Eq[logp - logq]
        logpq = log_p_zx_density(data_tuple, z_tuple, model_params_tuple)
        part1 = np.mean(logpq)
        
        part2_z_q = norm.logpdf(z_q.T, var_params_q[:, 0], np.exp(var_params_q[:, 1]))
        part2_z_e = norm.logpdf(z_e_, 
                                var_params_e[:, :, 0].flatten(), 
                                np.exp(e_log_std.flatten()))
        part2     = np.sum(part2_z_q, axis=-1) + np.sum(part2_z_e, axis=-1)
        part2     = np.mean(part2)
        
         # 求ELBO最大, 所以这里加个负号即minimize
        lower_bound = part1 - part2 
        return -lower_bound
                            
    
    # 变分参数
    var_params_q, var_params_e = var_params
    q_shape                    = var_params_q.shape
    e_shape                    = var_params_e.shape
    aid_num, q_dim             = q_shape
    aid_num, pi_num, e_dim     = e_shape
    var_params_q_              = var_params_q.flatten()
    var_params_e_              = var_params_e.flatten()
    len_q                      = len(var_params_q_)
    len_e                      = len(var_params_e_)
    var_params_cat             = np.concatenate([var_params_q_, var_params_e_])
    
    # 观测数据
    x_list  = list()
    pi_list = list()
    for k in data:
        x_k = data[k]['x']
        pi_k = data[k]['pi']
        x_list.append(x_k)
        pi_list.append(pi_k)
    
    # 梯度下降更新变分参数 - q & e
    gradient          = grad(variational_objective)
    # gradient(model_params, 0)
    model_params_next = adam(gradient, model_params, step_size=step_size, num_iters=num_iters)
    
    model_params_next[: pi_num, 1] = np.maximum(model_params_next[: pi_num, 1], -5)
    return model_params_next


#%%
def create_simulation_data(model_params_real, sampling_params, variant=''):
    
    # 抽样参数
    aid_num, nop_num, pi_num, prob_pi = sampling_params
    # 模型参数
    model_params_e = model_params_real[:pi_num]
    mu_Q, log_sig_Q = model_params_real[pi_num: ][0]
    mu_P, log_sig_P = model_params_real[pi_num: ][1]

    # 抽取每位作者的Q值
    if variant in ['f', 'chi2', 't']:
        if variant == 't':
           q_obs = sampling_t(10, aid_num)
        if variant == 'chi2':
           q_obs = sampling_chi2(1, aid_num)
        if variant == 'f':
           q_obs = sampling_f(5, 10, aid_num)
    else:
        q_obs = sampling_normal(mu_Q, log_sig_Q, aid_num)
    # 抽取每位作者的(第一作者)发文量 - unique papers
    nop_obs = np.maximum(sampling_poisson(nop_num, aid_num), 1)
    
    # 抽取每篇文章的P值
    p_obs = sampling_normal(mu_P, log_sig_P, np.sum(nop_obs)).squeeze()
    
    # 抽取每篇文章所属策略: 总发文量数目 x 总策略数目 
    pi_obs = list()
    for prob_pi_v in prob_pi:  # 第v个策略发生的概率    
        M_pi_v = smapling_binomial(1, prob_pi_v, np.sum(nop_obs))  # 两点分布
        pi_obs.append(M_pi_v)
    pi_obs = np.array(pi_obs).T
    
    # 抽取每个人执行策略的效率: 总人员数目 x 总策略数目
    e_obs = list()
    for pi_v in model_params_e:
        l, h  = pi_v
        
        if variant in ['f', 'chi2', 't']:
            if variant == 't':
               M_e_v = sampling_t(5, aid_num)
            if variant == 'chi2':
               M_e_v = sampling_chi2(1, aid_num)
            if variant == 'f':
               M_e_v = sampling_f(5, 10, aid_num)  
        else:
            M_e_v = sampling_normal(l, h, aid_num)
    
        e_obs.append(M_e_v.flatten())
    e_obs = np.array(e_obs).T
     
    data  = dict()
    start = 0
    end   = 0
    for k in range(aid_num):
        data[k] = dict()
        q_k     = q_obs[k]
        e_k     = e_obs[k]
        nop_k   = nop_obs[k]
        end     = start + nop_k
        p_k     = p_obs[start: end]
        pi_k    = pi_obs[start: end]
        start   = end
        data[k]['nop'] = nop_k  # 学者k的发文量
        data[k]['pi']  = pi_k   # 每篇文章的策略执行情况
        data[k]['q']   = q_k    # 学者k的科研能力
        data[k]['e']   = e_k    # 学者k的策略执行效率
        data[k]['p']   = p_k    # 每篇文章的引用随机性
        
    # 计算 x_obs
    for k in range(aid_num):
        pi_k = data[k]['pi']    # 观测数据
        e_k  = data[k]['e']     # 隐藏随机变量 (感兴趣)
        q_k  = data[k]['q']     # 隐藏随机变量 (感兴趣)
        p_k  = data[k]['p']     # 隐藏随机变量 (不感兴趣)
        
        M_pi_e = pi_k * e_k
        M_pi_e = np.mean(M_pi_e, axis=-1)
        qe_k   = q_k  + M_pi_e
        x_k    = qe_k + p_k 
        
        data[k]['x'] = x_k      # 观测数据
    
    return data


def max_likelihoood(data):
    # 极大似然估计q, 估计e
    x_obs = list()
    for k in data:
        x_k = data[k]['x']
        x_obs.append(x_k)
    x_obs = np.concatenate(x_obs, axis=0)   
    std   = np.std(x_obs)
    
    # 极大似然估计, 估计隐变量q 和 e
    var_params_q = list()
    var_params_e = list()
    for k in data:
        x_k       = data[k]['x']
        _, pi_num = data[k]['pi'].shape
        
        # 估计科研能力q
        q_mu      = np.mean(x_k)
        q_log_std = np.log(max(std - np.std(x_k), 1e-2))
        var_params_q_k = [q_mu, q_log_std]
        var_params_q.append(var_params_q_k)
        
        # 估计选题效率e
        var_params_e_k = np.ones((pi_num, 1)) * np.array([0, 0])
        var_params_e_k = var_params_e_k + np.random.normal(size=var_params_e_k.shape)
        var_params_e.append(var_params_e_k)
        
    var_params_q   = np.array(var_params_q)
    var_params_e   = np.array(var_params_e)
    var_params_est = (var_params_q, var_params_e)
    
    # 估计模型参数 
    mu_Q                = np.mean(var_params_q[:, 0])
    mu_P                = 0.0
    log_sig_Q           = np.log(max(np.std(var_params_q[:, 0]), 1e-2))
    log_sig_P           = np.log(max(std, 1e-2))
    model_params_e_est  = np.ones((pi_num, 1)) * np.array([0, 0])
    model_params_QP_est = np.array([[mu_Q, log_sig_Q], [mu_P, log_sig_P]])
    model_params_est    = np.concatenate([model_params_e_est, model_params_QP_est], axis=0)

    return var_params_est, model_params_est


def evaluate_on_simulation_data(data, model_params_real, var_params_est, model_params_est):
    
    def evaluate_real2pred(Y, X):
        # 评价指标:  Y是真实值, X是预测值
        cor, pvalue = pearsonr(Y, X)
        rmse = np.sqrt(mean_squared_error(Y, X))
        mae  = mean_absolute_error(Y, X)
        r2   = r2_score(Y, X)
        return cor, rmse, mae, r2
        
    def plot_q(q_real, q_real_sig, q_var, q_var_err, xlabel, ylabel, legend_1, legend_2, title):
        fig = plt.figure(figsize=(10, 8))
        plt.rcParams['savefig.dpi'] = 300
        plt.rcParams['figure.dpi'] = 300
        config = {
                  "font.family" : "SimHei",
                  "font.size" : 22
                  }
        rcParams.update(config)
        plt.rcParams['axes.unicode_minus'] = False # SimHei 字体符号不正常显示
   
        fontsize = 22
        
        plt.plot(np.arange(len(q_real)) +1, q_real, 
                 label=legend_1, c='red', marker='s', alpha=0.5, linewidth=1)
        plt.errorbar(np.arange(len(q_real))+1, q_var, yerr=q_var_err, 
                     label=legend_2, fmt="o:", color='blue', ecolor='dimgray', capsize=5, markersize=5, elinewidth=0.8)
        plt.ylabel(ylabel, fontsize=fontsize)
        plt.xlabel(xlabel, fontsize=fontsize)
        plt.legend(frameon=False, loc='upper right', fontsize=fontsize)
        plt.title(title, fontsize=fontsize)
        plt.yticks(np.arange(-4, 5, 1))
         
    def plot_e(M_e, v, M_e_var, M_e_var_err, xlabel, ylabel, legend_1, legend_2, title):
        fig = plt.figure(figsize=(10, 8))
        plt.rcParams['savefig.dpi'] = 300
        plt.rcParams['figure.dpi'] = 300
        config = {
                  "font.family" : "SimHei", # Times New Roman
                  "font.size" : 22
                  }
        rcParams.update(config)
        plt.rcParams['axes.unicode_minus'] = False # SimHei 字体符号不正常显示
        
        fontsize      = 22
        c             = ["brown", "green", "purple", "orange"]
        M_e_v         = M_e[:, v]
        M_e_var_v     = M_e_var[:, v]
        M_e_var_err_v = M_e_var_err[:, v]
        
        plt.plot(np.arange(len(M_e_v)) +1, M_e_v, 
                 label=legend_1, c=c[v], marker='s', alpha=0.5, linewidth=1)
        plt.errorbar(np.arange(len(M_e_var_v)) +1, M_e_var_v, yerr=M_e_var_err_v,
                     label=legend_2, fmt="o:", color='blue', ecolor='dimgray', capsize=5, markersize=5, elinewidth=0.8)
                      
        plt.ylabel(ylabel, fontsize=fontsize)
        plt.xlabel(xlabel, fontsize=fontsize)
        plt.legend(frameon=False, loc='upper right', fontsize=fontsize)
        plt.title(title, fontsize=fontsize)
        
        up   = np.round(np.mean(M_e_v)) + 3 * 1
        down = np.round(np.mean(M_e_v)) - 3 * 1
        plt.yticks(np.arange(down, up + 1, 1))
        plt.ylim(down, up)

    
    # 生成的科研能力和策略效率
    q_real = list()
    M_e    = list()
    for k in data:
        q_k = data[k]["q"]
        e_k = data[k]["e"]
        q_real.append(q_k)
        M_e.append(e_k)
    q_real = np.array(q_real)
    M_e    = np.array(M_e)
    
    # 科研能力变分参数 和 策略效率变分参数
    var_params_q, var_params_e = var_params_est
    # 策略效率变分参数
    M_e_var     = var_params_e[:, :, 0]
    M_e_var_err = np.exp(var_params_e[:, :, 1])
    # M_e_var_err = np.zeros(M_e_var_err.shape)
    
    results_E = list()
    tb = pt.PrettyTable()
    tb.field_names = ["*", "Pearsonr", "R2", "RMSE", "MAE"]
    aid_num, pi_num = M_e.shape
    for v in range(pi_num):
        # plot_e(M_e, v, M_e_var, M_e_var_err, "Scientists", r"$\hat{E}$" + "{}".format(v), 'Real', 'Estimated')
        plot_e(M_e, v, M_e_var, M_e_var_err, "学者", "策略效率" + r"($E_{}$)".format(v+1), '真实值', '估计值', "选题策略模型 (模拟配置12)")
        cor_e, rmse_e, mae_e, r2_e = evaluate_real2pred(M_e[:, v], M_e_var[:, v])
        tb.add_row(["策略效率", "{:.4f}".format(cor_e), "{:.4f}".format(r2_e), "{:.4f}".format(rmse_e), "{:.4f}".format(mae_e)])
        results_E.append((cor_e, r2_e, rmse_e, mae_e))
        
    # 人员q3值 (errorbar表示估计的精度)
    q_real = q_real.flatten()
    q_est, q_est_err = var_params_q[:, 0], var_params_q[:, 1]
    # plot_q(q_real, 0, q_est, np.exp(q_est_err), "学者", r"$\hat{Q}$", '真实值', '估计值')
    plot_q(q_real, 0, q_est, np.exp(q_est_err), "学者", "学者科研能力", '真实值', '估计值', "选题策略模型 (模拟配置12)")
    cor_q, rmse_q, mae_q, r2_q = evaluate_real2pred(q_real, q_est)
    tb.add_row(["研究能力", "{:.4f}".format(cor_q), "{:.4f}".format(r2_q), "{:.4f}".format(rmse_q), "{:.4f}".format(mae_q)])
    print(tb)
    
    return results_E , (cor_q, r2_q, rmse_q, mae_q)
    

#%%
np.set_printoptions(precision=6, suppress=True)
def BBVI_Algorithm(variant=''):
    aid_num = 500                       # 人员数目
    nop_num = 30                        # 服从泊松分布 poisson(nop_num) 每位作者的平均发文量数目
    # prob_pi = [0.5, 0.5, 0.5]           # 两点分布抽取策略发生概率
    # prob_pi = [0.5, 0.5]
    prob_pi = [0.5]
    pi_num  = len(prob_pi)              # 策略种类
    sampling_params = [aid_num, nop_num, pi_num, prob_pi]
    
    # model_params_e = np.array([[2, 0], [-2, 0], [0, 0]])
    # model_params_e = np.array([[2, 0], [-2, 0]])
    model_params_e = np.array([[2, 0]])
    
    mu_Q_real, log_sig_Q_real = 0.0, 0.0
    mu_P_real, log_sig_P_real = 0.0, -1
    model_params_q1 = np.array([[mu_Q_real, log_sig_Q_real]])   # q先验
    model_params_P = np.array([[mu_P_real, log_sig_P_real]])    # 引用随机性参数
    model_params_real = np.concatenate([model_params_e, model_params_q1, model_params_P], axis=0)
    
    # 采样模拟数据
    data = create_simulation_data(model_params_real, sampling_params, variant)
   
    # 极大似然估计 - 求平均
    var_params_est, model_params_est = max_likelihoood(data)
    
    # 贝叶斯后验估计
    Epochs      = 20
    step_size   = 1e-1
    num_iters   = 100
    num_samples = 1
    var_params, model_params = var_params_est, model_params_est
    for e in range(Epochs):
        # E-Step
        print("({}) Optimizing variational parameters...".format(e))           
        E_start_time    = time.perf_counter()
        var_params_next = Estep(data, model_params, var_params, num_samples, step_size, num_iters)
        E_end_time      = time.perf_counter()                      
        var_params      = var_params_next
        print("Estep 耗时: {:.4f}".format(E_end_time-E_start_time))
    
        # M-Step
        print("({}) Optimizing model parameters...".format(e))
        M_start_time      = time.perf_counter()
        model_params_next = Mstep(data, model_params, var_params, num_samples, step_size, num_iters)
        M_end_time        = time.perf_counter()
        model_params      = model_params_next
        print("Estep 耗时: {:.4f}".format(M_end_time-M_start_time))
        
    # 变分参数估计 - BBVI-EM
    var_params_bbvi, model_params_bbvi = var_params, model_params
    var_params_est,  model_params_est  = max_likelihoood(data)
    
    e_sin, q_sin = evaluate_on_simulation_data(data, model_params_real, var_params_est,  model_params_est)
    e_our, q_our = evaluate_on_simulation_data(data, model_params_real, var_params_bbvi, model_params_bbvi)

    return e_sin, q_sin, e_our, q_our


def get_list(results, Key):
    # 计算每个指标的均值; t检验
    e1_eval_list = list()
    e2_eval_list = list()
    e3_eval_list = list()
    e_eval_list_dict = {"1": e1_eval_list, 
                        "2": e2_eval_list, 
                        "3": e3_eval_list}
    q_eval_list  = list()
    for j in results:
        e_our, q_our = results[j][Key]
        q_eval_list.append(list(q_our))
        for i in range(len(e_our)):
            e_eval_list_dict[str(i + 1)].append(list(e_our[i]))
    
    # R2可为负, 置0
    for i in range(len(e_our)):
        e_eval_list_dict[str(i + 1)] = np.maximum(e_eval_list_dict[str(i + 1)], 0)  
    q_eval_list = np.maximum(np.array(q_eval_list),  0)
    return e_eval_list_dict, q_eval_list


def main():
    times = 5
    number = 1
    results = dict()
    for i in range(times):
        e_sin, q_sin, e_our, q_our = BBVI_Algorithm('')
        results[i] = dict()
        results[i]["wsb"] = (e_sin, q_sin)
        results[i]["org"] = (e_our, q_our)

    _, q_WSB_list = get_list(results, "wsb")
    e_eval_list_dict, q_OUR_list = get_list(results, "org")
    
    # 科研能力比较
    q_WSB_mean = np.mean(q_WSB_list, axis=0)
    q_OUR_mean = np.mean(q_OUR_list, axis=0)
    print("WSB: Pearsonr: {:.4f}, R2: {:.4f}, RMES: {:.4f}, MAE: {:.4f}".format(*q_WSB_mean))
    print("OUR: Pearsonr: {:.4f}, R2: {:.4f}, RMSE: {:.4f}, MAE: {:.4f}".format(*q_OUR_mean))
    for i in range(4):
        _, pvalue = ttest_rel(q_WSB_list[:, i], q_OUR_list[:, i])
        print("{:.6f}".format(pvalue))
    
    # 选题策略比较
    for i in e_eval_list_dict:
        if np.array(e_eval_list_dict[i]).any():
            e1_OUR_mean = np.mean(e_eval_list_dict[i], axis=0)
            print("OUR: Pearsonr: {:.4f}, R2: {:.4f}, RMSE: {:.4f}, MAE: {:.4f}".format(*e1_OUR_mean))
    
    
    
        
